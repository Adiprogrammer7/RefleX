{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today was a beautiful sunny day. I went for a long walk in the park and enjoyed the fresh air and the sound of birds chirping. It was so refreshing and calming. I also had a chance to sit by the lake and read a book. The book was a captivating mystery novel that kept me hooked from beginning to end. I love how books can transport me to different worlds and make me forget about everything else for a while. Reading is such a wonderful way to relax and unwind. This evening, I met up with my friends for dinner at a new restaurant in town. We had a great time catching up, laughing, and sharing stories. The food was delicious, and the atmosphere was lively. It's always nice to spend quality time with friends and create beautiful memories together. We talked about our upcoming plans for a hiking trip and shared recommendations for books and movies. I can't wait to explore new trails and discover more intriguing stories. Overall, it was a day filled with joy, relaxation, and meaningful connections.\n"
     ]
    }
   ],
   "source": [
    "content = \"Today was a beautiful sunny day. I went for a long walk in the park and enjoyed the fresh air and the sound of birds chirping. It was so refreshing and calming. I also had a chance to sit by the lake and read a book. The book was a captivating mystery novel that kept me hooked from beginning to end. I love how books can transport me to different worlds and make me forget about everything else for a while. Reading is such a wonderful way to relax and unwind. This evening, I met up with my friends for dinner at a new restaurant in town. We had a great time catching up, laughing, and sharing stories. The food was delicious, and the atmosphere was lively. It's always nice to spend quality time with friends and create beautiful memories together. We talked about our upcoming plans for a hiking trip and shared recommendations for books and movies. I can't wait to explore new trails and discover more intriguing stories. Overall, it was a day filled with joy, relaxation, and meaningful connections.\"\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(content):\n",
    "    # Convert the text to lowercase\n",
    "    content = content.lower()\n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    tokens = word_tokenize(content)\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a preprocessed text\n",
    "    preprocessed_content = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(preprocessed_content, top_n=5):\n",
    "    # Tokenize the preprocessed content into individual words\n",
    "    tokens = word_tokenize(preprocessed_content)\n",
    "    \n",
    "    # Calculate the frequency distribution of the tokens\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    \n",
    "    # Extract the top n most frequent keywords\n",
    "    keywords = [token for token, freq in freq_dist.most_common(top_n)]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_keywords_ner(preprocessed_content, entity_types=['PERSON', 'ORG', 'GPE'], top_n=5):\n",
    "#     # Tokenize the preprocessed content into individual words\n",
    "#     tokens = word_tokenize(preprocessed_content)\n",
    "    \n",
    "#     # Perform named entity recognition on the tokens\n",
    "#     tagged_entities = nltk.ne_chunk(nltk.pos_tag(tokens), binary=False)\n",
    "    \n",
    "#     # Extract the words with the specified named entity types\n",
    "#     keywords = [leaf[0] for leaf in tagged_entities if isinstance(leaf, nltk.Tree) and leaf.label() in entity_types]\n",
    "    \n",
    "#     # Extract the top n most frequent keywords\n",
    "#     freq_dist = FreqDist(keywords)\n",
    "#     keywords = [token for token, freq in freq_dist.most_common(top_n)]\n",
    "    \n",
    "#     return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today+beautiful+sunny+day+went+long+walk+park+enjoyed+fresh+air+sound+bird+chirping+refreshing+calming+also+chance+sit+lake+read+book+book+captivating+mystery+novel+kept+hooked+beginning+end+love+book+transport+different+world+make+forget+everything+else+reading+wonderful+way+relax+unwind+evening+met+friend+dinner+new+restaurant+town+great+time+catching+laughing+sharing+story+food+delicious+atmosphere+lively+'s+always+nice+spend+quality+time+friend+create+beautiful+memory+together+talked+upcoming+plan+hiking+trip+shared+recommendation+book+movie+ca+n't+wait+explore+new+trail+discover+intriguing+story+overall+day+filled+joy+relaxation+meaningful+connection\n"
     ]
    }
   ],
   "source": [
    "preprocessed_content = preprocess_content(content)\n",
    "print(preprocessed_content.replace(\" \", \"+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'beautiful', 'day', 'friend', 'new']\n"
     ]
    }
   ],
   "source": [
    "extracted_keywords = extract_keywords(preprocessed_content)\n",
    "print(extracted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rake_nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e3efb5290dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrake_nltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Uses stopwords for english from NLTK, and all puntuation characters by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rake_nltk'"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by\n",
    "# default\n",
    "r = Rake()\n",
    "\n",
    "# Extraction given the text.\n",
    "r.extract_keywords_from_text(preprocessed_content)\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest.\n",
    "print(r.get_ranked_phrases())\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest with scores.\n",
    "print(r.get_ranked_phrases_with_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
